{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34588011",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HERE DATA SET ARE TRAINED \n",
    "\n",
    "# import csv\n",
    "# import array\n",
    "# import pandas\n",
    "# import pickle\n",
    "# import os\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import svm\n",
    "# csvFile=open('newfrequency300.csv', 'rt')\n",
    "# csvReader=csv.reader(csvFile)\n",
    "# mydict={row[1]: int(row[0]) for row in csvReader}\n",
    "\n",
    "# y=[]\n",
    "# with open ('PJFinaltest.csv', 'rt') as f:\n",
    "# \treader=csv.reader(f)\n",
    "# \tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "# with open ('PJFinaltest.csv', 'rt') as f:\n",
    "# \tcsvReader1=csv.reader(f)\n",
    "# \tfor rows in csvReader1:\n",
    "# \t\ty.append([int(rows[1])])\n",
    "# vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "# x=vectorizer.fit_transform(corpus).toarray()\n",
    "# result=np.append(x,y,axis=1)\n",
    "# X=pandas.DataFrame(result)\n",
    "# model=GaussianNB()\n",
    "# train = X.sample(frac=0.8, random_state=1)\n",
    "# test=X.drop(train.index)\n",
    "# y_train=train[301]\n",
    "# y_test=test[301]\n",
    "# print(train.shape)\n",
    "# print(test.shape)\n",
    "# xtrain=train.drop(301,axis=1)\n",
    "# xtest=test.drop(301,axis=1)\n",
    "# model.fit(xtrain,y_train)\n",
    "# pickle.dump(model, open('BNPJFinal.sav', 'wb'))\n",
    "# del result\n",
    "\n",
    "# y=[]\n",
    "# with open ('IEFinaltest.csv', 'rt') as f:\n",
    "# \treader=csv.reader(f)\n",
    "# \tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "# with open ('IEFinaltest.csv', 'rt') as f:\n",
    "# \tcsvReader1=csv.reader(f)\n",
    "# \tfor rows in csvReader1:\n",
    "# \t\ty.append([int(rows[1])])\n",
    "# vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "# x=vectorizer.fit_transform(corpus).toarray()\n",
    "# result=np.append(x,y,axis=1)\n",
    "# X=pandas.DataFrame(result)\n",
    "# model=GaussianNB()\n",
    "# train = X.sample(frac=0.8, random_state=1)\n",
    "# test=X.drop(train.index)\n",
    "# y_train=train[301]\n",
    "# y_test=test[301]\n",
    "# print(train.shape)\n",
    "# print(test.shape)\n",
    "# xtrain=train.drop(301,axis=1)\n",
    "# xtest=test.drop(301,axis=1)\n",
    "# model.fit(xtrain,y_train)\n",
    "# pickle.dump(model, open('BNIEFinal.sav', 'wb'))\n",
    "# del result\n",
    "\n",
    "# y=[]\n",
    "# with open ('TFFinaltest.csv', 'rt') as f:\n",
    "# \treader=csv.reader(f)\n",
    "# \tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "# with open ('TFFinaltest.csv', 'rt') as f:\n",
    "# \tcsvReader1=csv.reader(f)\n",
    "# \tfor rows in csvReader1:\n",
    "# \t\ty.append([int(rows[1])])\n",
    "# vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "# x=vectorizer.fit_transform(corpus).toarray()\n",
    "# result=np.append(x,y,axis=1)\n",
    "# X=pandas.DataFrame(result)\n",
    "# model=GaussianNB()\n",
    "# train = X.sample(frac=0.8, random_state=1)\n",
    "# test=X.drop(train.index)\n",
    "# y_train=train[301]\n",
    "# y_test=test[301]\n",
    "# print(train.shape)\n",
    "# print(test.shape)\n",
    "# xtrain=train.drop(301,axis=1)\n",
    "# xtest=test.drop(301,axis=1)\n",
    "# model.fit(xtrain,y_train)\n",
    "# pickle.dump(model, open('BNTFFinal.sav', 'wb'))\n",
    "# del result\n",
    "\n",
    "# y=[]\n",
    "# with open ('SNFinaltest.csv', 'rt') as f:\n",
    "# \treader=csv.reader(f)\n",
    "# \tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "# with open ('SNFinaltest.csv', 'rt') as f:\n",
    "# \tcsvReader1=csv.reader(f)\n",
    "# \tfor rows in csvReader1:\n",
    "# \t\ty.append([int(rows[1])])\n",
    "# vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1,lowercase=False)\n",
    "# x=vectorizer.fit_transform(corpus).toarray()\n",
    "# result=np.append(x,y,axis=1)\n",
    "# X=pandas.DataFrame(result)\n",
    "# model=GaussianNB()\n",
    "# train = X.sample(frac=0.8, random_state=1)\n",
    "# test=X.drop(train.index)\n",
    "# y_train=train[301]\n",
    "# y_test=test[301]\n",
    "# print(train.shape)\n",
    "# print(test.shape)\n",
    "# xtrain=train.drop(301,axis=1)\n",
    "# xtest=test.drop(301,axis=1)\n",
    "# model.fit(xtrain,y_train)\n",
    "# pickle.dump(model, open('BNSNFinal.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53377692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sanje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sanje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HERE REAL DATA ARE TESTED\n",
    "\n",
    "\n",
    "import tweepy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import sys\n",
    "import os\n",
    "import nltk \n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import csv\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4ccebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def preproc(s):\n",
    "\t#s=emoji_pattern.sub(r'', s) # no emoji\n",
    "\ts= unidecode(s)\n",
    "\tPOSTagger=preprocess(s)\n",
    "\t#print(POSTagger)\n",
    "\n",
    "\ttweet=' '.join(POSTagger)\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\tword_tokens = word_tokenize(tweet)\n",
    "\t#filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\tfiltered_sentence = []\n",
    "\tfor w in POSTagger:\n",
    "\t    if w not in stop_words:\n",
    "\t        filtered_sentence.append(w)\n",
    "\t#print(word_tokens)\n",
    "\t#print(filtered_sentence)\n",
    "\tstemmed_sentence=[]\n",
    "\tstemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\tfor w in filtered_sentence:\n",
    "\t\tstemmed_sentence.append(stemmer2.stem(w))\n",
    "\t#print(stemmed_sentence)\n",
    "\n",
    "\ttemp = ' '.join(c for c in stemmed_sentence if c not in string.punctuation) \n",
    "\tpreProcessed=temp.split(\" \")\n",
    "\tfinal=[]\n",
    "\tfor i in preProcessed:\n",
    "\t\tif i not in final:\n",
    "\t\t\tif i.isdigit():\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tif 'http' not in i:\n",
    "\t\t\t\t\tfinal.append(i)\n",
    "\ttemp1=' '.join(c for c in final)\n",
    "\t#print(preProcessed)\n",
    "\treturn temp1\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd3b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................completed\n",
      "ISTJ - The Organizer: Practical, reliable, likes order.\n",
      "ISFJ - The Helper: Caring, supportive, puts others first.\n",
      "INFJ - The Counselor: Empathetic, idealistic, strives to make a positive impact.\n",
      "INTJ - The Strategist: Logical, analytical, excellent planner.\n",
      "ISTP - The Adventurer: Flexible, hands-on, enjoys risks.\n",
      "ISFP - The Artist: Creative, free-spirited, values self-expression.\n",
      "INFP - The Dreamer: Kind, imaginative, seeks meaning in life.\n",
      "INTP - The Thinker: Curious, logical, loves solving problems.\n",
      "ESTP - The Dynamo: Energetic, action-oriented, thrill-seeker.\n",
      "ESFP - The Performer: Social, outgoing, loves the spotlight.\n",
      "ENFP - The Inspirer: Enthusiastic, creative, inspires others.\n",
      "ENTP - The Visionary: Innovative, quick thinker, explores new ideas.\n",
      "ESTJ - The Supervisor: Organized, dependable, values structure.\n",
      "ESFJ - The Provider: Warm, caring, dedicated to helping others.\n",
      "ENFJ - The Teacher: Charismatic, empathetic leader, guides and uplifts others.\n",
      "ENTJ - The Commander: Confident, goal-oriented leader, enjoys taking charge.\n",
      "[(0.0, 172)]\n",
      "[(1.0, 149)]\n",
      "[(1.0, 157)]\n",
      "[(0.0, 140)]\n",
      "ESTJ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''  *********************  for narendramodi    ************  ''' \n",
    "\n",
    "\n",
    "\n",
    "def getTweets():\n",
    "    csvFile = open('user.csv', 'w', newline='')  # Open 'user.csv' in write mode\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    i = 0\n",
    "    try:\n",
    "        with open('narendramodi.csv', 'r', newline='', encoding='utf-8') as csvFile:  # Specify the encoding as 'utf-8'\n",
    "            csvReader = csv.reader(csvFile)\n",
    "            for row in csvReader:\n",
    "                if i == 270:\n",
    "                    break\n",
    "                i += 1\n",
    "                tw = preproc(row[10])  # Corrected variable and method calls\n",
    "                tw = re.sub(' +', ' ', tw)  # Replace multiple consecutive spaces with a single space\n",
    "#                 print(tw)\n",
    "                csvWriter.writerow([tw])\n",
    "    except FileNotFoundError:\n",
    "        print(\"File 'narendramodi.csv' not found.\")\n",
    "    finally:\n",
    "        csvFile.close()\n",
    "\n",
    "\n",
    "\n",
    "'''     *******************************   Sanjeev Pratap **************************       '''\n",
    "# def getTweets():\n",
    "#     csvFile = open('user.csv', 'w', newline='')  # Open 'user.csv' in write mode\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     i = 0\n",
    "#     try:\n",
    "#         with open('messages.csv', 'r', newline='', encoding='utf-8') as csvFile:  # Specify the encoding as 'utf-8'\n",
    "#             csvReader = csv.reader(csvFile)\n",
    "#             for row in csvReader:\n",
    "                \n",
    "#                 tw = preproc(row[8])  # Corrected variable and method calls\n",
    "#                 tw = re.sub(' +', ' ', tw)  # Replace multiple consecutive spaces with a single space\n",
    "#                 #print(tw)\n",
    "#                 csvWriter.writerow([tw])\n",
    "#         with open('skills.csv', 'r', newline='', encoding='utf-8') as csvFile:  # Specify the encoding as 'utf-8'\n",
    "#             csvReader = csv.reader(csvFile)\n",
    "#             for row in csvReader:\n",
    "                \n",
    "#                 tw = preproc(row[0])  # Corrected variable and method calls\n",
    "#                 tw = re.sub(' +', ' ', tw)  # Replace multiple consecutive spaces with a single space\n",
    "#                 #print(tw)\n",
    "#                 csvWriter.writerow([tw])\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"File 'narendramodi.csv' not found.\")\n",
    "#     finally:\n",
    "#         csvFile.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "getTweets()\n",
    "print(\"..........................................................completed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mbti_types = [\n",
    "    (\"ISTJ\", \"The Organizer: Practical, reliable, likes order.\"),\n",
    "    (\"ISFJ\", \"The Helper: Caring, supportive, puts others first.\"),\n",
    "    (\"INFJ\", \"The Counselor: Empathetic, idealistic, strives to make a positive impact.\"),\n",
    "    (\"INTJ\", \"The Strategist: Logical, analytical, excellent planner.\"),\n",
    "    (\"ISTP\", \"The Adventurer: Flexible, hands-on, enjoys risks.\"),\n",
    "    (\"ISFP\", \"The Artist: Creative, free-spirited, values self-expression.\"),\n",
    "    (\"INFP\", \"The Dreamer: Kind, imaginative, seeks meaning in life.\"),\n",
    "    (\"INTP\", \"The Thinker: Curious, logical, loves solving problems.\"),\n",
    "    (\"ESTP\", \"The Dynamo: Energetic, action-oriented, thrill-seeker.\"),\n",
    "    (\"ESFP\", \"The Performer: Social, outgoing, loves the spotlight.\"),\n",
    "    (\"ENFP\", \"The Inspirer: Enthusiastic, creative, inspires others.\"),\n",
    "    (\"ENTP\", \"The Visionary: Innovative, quick thinker, explores new ideas.\"),\n",
    "    (\"ESTJ\", \"The Supervisor: Organized, dependable, values structure.\"),\n",
    "    (\"ESFJ\", \"The Provider: Warm, caring, dedicated to helping others.\"),\n",
    "    (\"ENFJ\", \"The Teacher: Charismatic, empathetic leader, guides and uplifts others.\"),\n",
    "    (\"ENTJ\", \"The Commander: Confident, goal-oriented leader, enjoys taking charge.\")\n",
    "]\n",
    "\n",
    "for mbti_type, description in mbti_types:\n",
    "    print(f\"{mbti_type} - {description}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('user.csv','rt') as f:\n",
    "\tcsvReader=csv.reader(f)\n",
    "\ttweetList=[rows[0] for rows in csvReader]\n",
    "# print(tweetList,\".....................\")\n",
    "with open('newfrequency300.csv','rt') as f:\n",
    "\tcsvReader=csv.reader(f)\n",
    "\tmydict={rows[1]: int(rows[0]) for rows in csvReader}\n",
    "\n",
    "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1,lowercase=False)\n",
    "x=vectorizer.fit_transform(tweetList).toarray()\n",
    "df=pd.DataFrame(x)\n",
    "\n",
    "\n",
    "model_IE = pickle.load(open(\"BNIEFinal.sav\", 'rb'))\n",
    "model_SN = pickle.load(open(\"BNSNFinal.sav\", 'rb'))\n",
    "model_TF = pickle.load(open('BNTFFinal.sav', 'rb'))\n",
    "model_PJ = pickle.load(open('BNPJFinal.sav', 'rb'))\n",
    "\n",
    "answer=[]\n",
    "IE=model_IE.predict(df)\n",
    "SN=model_SN.predict(df)\n",
    "TF=model_TF.predict(df)\n",
    "PJ=model_PJ.predict(df)\n",
    "\n",
    "\n",
    "b = Counter(IE)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1.0:\n",
    "\tanswer.append(\"I\")\n",
    "else:\n",
    "\tanswer.append(\"E\")\n",
    "\n",
    "b = Counter(SN)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1.0:\n",
    "\tanswer.append(\"S\")\n",
    "else:\n",
    "\tanswer.append(\"N\")\n",
    "\n",
    "b = Counter(TF)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1:\n",
    "\tanswer.append(\"T\")\n",
    "else:\n",
    "\tanswer.append(\"F\")\n",
    "\n",
    "b = Counter(PJ)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1:\n",
    "\tanswer.append(\"P\")\n",
    "else:\n",
    "\tanswer.append(\"J\")\n",
    "mbti=\"\".join(answer)\n",
    "print(mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48dd757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f678b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
